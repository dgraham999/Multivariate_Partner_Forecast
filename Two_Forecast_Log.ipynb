{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program 2 SALES FORECASTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PURPOSE:  \n",
    "This program implements a embedded neural network in tensorflow to perform a partner by partner\n",
    "sales forecast.  This is not an inferential program although it measures accuracy against a test set in mean \n",
    "average percentage error.  \n",
    "\n",
    "This is a demonstration/training program and is not production grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### INPUT: \n",
    "Original Data And features developed in the prior programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OUTPUT: \n",
    "Sales forecasts and accutacy on a test set of known data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import standard python and sklearn libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import os as os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tensorflow and tensorflow libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,callbacks,losses,optimizers,initializers,models,regularizers\n",
    "from tensorflow.keras.layers import Dense,Dropout,BatchNormalization,Embedding,Flatten,concatenate,Input\n",
    "from tensorflow.keras.callbacks import CSVLogger,ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error,mean_absolute_percentage_error\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop,Adam,Adamax\n",
    "from tensorflow.keras.initializers import RandomNormal,RandomUniform,TruncatedNormal\n",
    "from tensorflow.keras.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for gpu and expect this output:\n",
    "\n",
    "[\n",
    "  name: \"/cpu:0\"device_type: \"CPU\",\n",
    "  name: \"/gpu:0\"device_type: \"GPU\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3179301970092214904\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5910947058565456372\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_pickle(os.getcwd() + '/Partner_Features.pkl')\n",
    "dt.sort_values(by=['ID','Date'],inplace=True)\n",
    "dt.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify categorical, continuous, data and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_vars(dt):\n",
    "    cat_vars = list(dt.columns[:31])+list(dt.columns[-3:-1])\n",
    "    cont_vars = list(dt.columns[31:36])+list(dt.columns[36:50])+list(dt.columns[-7:-5])\n",
    "    dep = ['Rev']\n",
    "    date = ['Date']\n",
    "    dt = dt[cat_vars + cont_vars + dep + date].copy()\n",
    "    dt.sort_values(by=['ID','Date'],inplace=True)\n",
    "    dt.reset_index(drop=True,inplace=True)\n",
    "    return dt,cat_vars,cont_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,cat_vars,cont_vars = label_vars(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create categorical embed maximum length,embedding dict, and categorical map function of labelencoder to set number of categories in each category feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_data(df,cat_vars):\n",
    "    cat_emb_max = [len(df[c].unique()) for c in cat_vars]\n",
    "    cat_vars_dict = dict(zip(cat_vars,cat_emb_max))\n",
    "    cat_map = [(c,LabelEncoder()) for c in cat_vars]\n",
    "    return cat_vars_dict,cat_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_dict,cat_map = cat_data(df,cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create continuous data map function with minmaxwscaler and range default to 0,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_data(cont_vars,mn=0,mx=1):\n",
    "    cont_map = [([c],MinMaxScaler(feature_range = (mn,mx),copy=False)) for c in cont_vars]\n",
    "    return cont_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_map = cont_data(cont_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit map function to continuous and categorical features but do not apply transform until after data is split into train, validate and test.  This fits labels and scaled range to entire data set rather than train,validate and test separtely.  DataFrameMapper from sklearn-pandas will only transform the features by column label inluded in the category and continuous feature lists called cat_map or cont_map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_mapped(cat_map,cont_map,df):\n",
    "    cat_mapper = DataFrameMapper(cat_map)\n",
    "    cat_map_fit = cat_mapper.fit(df)\n",
    "    cont_mapper = DataFrameMapper(cont_map)\n",
    "    cont_map_fit = cont_mapper.fit(df)\n",
    "    return cat_map_fit,cont_map_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_map_fit,cont_map_fit = vars_mapped(cat_map,cont_map,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set train, test, validate sets with validation as one quarter of each year and test at last quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df,vstart=2,tstart=1):\n",
    "    dates = list(df.Date.unique())\n",
    "    dates.sort()\n",
    "    dates_validate = dates[-vstart:]\n",
    "    #dates_test = dates[-tstart:]\n",
    "    dates_train = dates[:-vstart]\n",
    "    data = df.sort_values(by=['ID','Date'])\n",
    "    data_train = data.loc[data.Date.isin(dates_train)]\n",
    "    data_validate = data.loc[data.Date.isin(dates_validate)]\n",
    "    return data_train,data_validate,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train,data_validate,data=split_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode and scale data and reshape into array of vectors. \n",
    "___________________________________________________________________________________________________________\n",
    "Since the input layer of the neural network is a horizontally concatenated layer of each categorical variable in its own embedding input shared with the continuous variables each in its own dense input the train, validate and test data needs to be reshaped into a list of vectors for each feature.  To keep the array in mixed dtypes (i.e., int and float), input data is a list of arrays with each element in the list being a vector for the shared input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_shape_data(data_train,data_validate,cat_map_fit,cont_map_fit):\n",
    "    #set target variables\n",
    "    y_tr = np.log(data_train.Rev.values).reshape(-1,1)\n",
    "    y_val = np.log(data_validate.Rev.values).reshape(-1,1)\n",
    "    #transform categorical data\n",
    "    cat_train = cat_map_fit.transform(data_train).astype(np.int64)\n",
    "    cat_validate = cat_map_fit.transform(data_validate).astype(np.int64)\n",
    "    #transform continuous variables\n",
    "    cont_train = cont_map_fit.transform(data_train).astype(np.float32)\n",
    "    cont_validate = cont_map_fit.transform(data_validate).astype(np.float32)\n",
    "    #combine categorical and continuous data into array of vectors\n",
    "    data_tr = np.hsplit(cat_train,cat_train.shape[1])+np.hsplit(cont_train,cont_train.shape[1])\n",
    "    data_val = np.hsplit(cat_validate,cat_validate.shape[1])+np.hsplit(cont_validate,cont_validate.shape[1])\n",
    "    return y_tr,y_val,data_tr,data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr,y_val,data_tr,data_val = map_shape_data(data_train,data_validate,cat_map_fit,cont_map_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create single input vector (input_shape = 1) for categorical input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_input(feat,cat_vars_dict):\n",
    "    name = feat[0]\n",
    "    c1 = len(feat[1].classes_)\n",
    "    c2 = cat_vars_dict[name]\n",
    "    if c2 > 50:c2 = 50\n",
    "    if c2 < 5:c2 = 5\n",
    "    inp = Input(shape=(1,),dtype='int64',name=name + '_in')\n",
    "    #no third dimension for a time distributed series so flattened into column of 1\n",
    "    #embedding layer is map of number of classes (c) to number of embedded features (c2)\n",
    "    u = Flatten(name=name+'_flt')(Embedding(c1,c2,input_length=1)(inp))\n",
    "    return inp,u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Input,Flatten,and Embedding layers for the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = [cat_input(feat,cat_vars_dict) for feat in cat_map_fit.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deprecation warning is an incompatibility between keras and tensorflow.keras.  The error message is an outstanding bug in tensorflow and does not occur in keras.  Tensorflow has an open issue report regarding this error message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to create Input and Dense layer for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_input(feat):\n",
    "    name = feat[0][0]\n",
    "    inp = Input((1,), name=name+'_in')\n",
    "    d = Dense(1, name = name + '_d')(inp)\n",
    "    return inp,d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of Input and Dense layers for continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = [cont_input(feat) for feat in cont_map_fit.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a four layer model using a shared input layer for the categorical and continuous variables.  The hideen 2 layers are high node counts because sample count in input data is large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_model(conts,embs):\n",
    "    #concatenate the inputs and embedded layers with the inputs and continuous dense layers\n",
    "    #referred to as 'shared layers' in tensorflow.keras documentation\n",
    "    x = concatenate([emb for inp,emb in embs] + [d for inp,d in conts])\n",
    "    #apply L2 normalization using the BatchNormalization method on continuous features\n",
    "    x = Dense(128, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    #apply small dropout for first normalization\n",
    "    x = Dropout(rate=0.6)(x)\n",
    "    #apply additional L2 normalization using the BatchNormalization method\n",
    "    x =\tBatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    #apply small dropout for normalization\n",
    "    x =\tDropout(rate=0.6)(x)\n",
    "    #apply L2 normalization using the BatchNormalization method\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64,activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)  \n",
    "    x =\tDropout(rate=0.6)(x)\n",
    "    #apply L2 normalization using the BatchNormalization method\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(1, activation='relu',kernel_initializer='uniform',bias_initializer='zeros')(x)\n",
    "    model = Model([inp for inp,emb in embs] + [inp for inp,d in conts], x)\n",
    "    model.compile(optimizer='Adam',loss='mean_absolute_error',metrics=['mape'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement logger,reduce the learning rate when loss function change gets small,add early stopping and build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('Partner_Error.csv')\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=5,min_lr=0.0001)\n",
    "mc = ModelCheckpoint('Partner_Best_Model',save_best_only=True)\n",
    "model = embed_model(conts,embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next process is cpu/gpu intensive.  This code should be run on a gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/25\n",
      "7000/7000 [==============================] - 20s 3ms/sample - loss: 1.9755 - mean_absolute_percentage_error: 73.8357 - val_loss: 0.9802 - val_mean_absolute_percentage_error: 29.2007\n",
      "Epoch 2/25\n",
      "7000/7000 [==============================] - 2s 261us/sample - loss: 0.7122 - mean_absolute_percentage_error: 31.9274 - val_loss: 0.4334 - val_mean_absolute_percentage_error: 14.8105\n",
      "Epoch 3/25\n",
      "7000/7000 [==============================] - 2s 266us/sample - loss: 0.4702 - mean_absolute_percentage_error: 22.2028 - val_loss: 0.1927 - val_mean_absolute_percentage_error: 7.6289\n",
      "Epoch 4/25\n",
      "7000/7000 [==============================] - 2s 251us/sample - loss: 0.4241 - mean_absolute_percentage_error: 20.3298 - val_loss: 0.1879 - val_mean_absolute_percentage_error: 7.2956\n",
      "Epoch 5/25\n",
      "7000/7000 [==============================] - 2s 260us/sample - loss: 0.3976 - mean_absolute_percentage_error: 19.1828 - val_loss: 0.1584 - val_mean_absolute_percentage_error: 6.0560\n",
      "Epoch 6/25\n",
      "7000/7000 [==============================] - 2s 219us/sample - loss: 0.3776 - mean_absolute_percentage_error: 18.1855 - val_loss: 0.3340 - val_mean_absolute_percentage_error: 11.6710\n",
      "Epoch 7/25\n",
      "7000/7000 [==============================] - 2s 259us/sample - loss: 0.3706 - mean_absolute_percentage_error: 17.6152 - val_loss: 0.2784 - val_mean_absolute_percentage_error: 9.8213\n",
      "Epoch 8/25\n",
      "7000/7000 [==============================] - 2s 266us/sample - loss: 0.3586 - mean_absolute_percentage_error: 17.4811 - val_loss: 0.0872 - val_mean_absolute_percentage_error: 3.3791\n",
      "Epoch 9/25\n",
      "7000/7000 [==============================] - 2s 238us/sample - loss: 0.3571 - mean_absolute_percentage_error: 17.1931 - val_loss: 0.1996 - val_mean_absolute_percentage_error: 7.1423\n",
      "Epoch 10/25\n",
      "7000/7000 [==============================] - 2s 255us/sample - loss: 0.3404 - mean_absolute_percentage_error: 16.2934 - val_loss: 0.1957 - val_mean_absolute_percentage_error: 7.0224\n",
      "Epoch 11/25\n",
      "7000/7000 [==============================] - 2s 237us/sample - loss: 0.3372 - mean_absolute_percentage_error: 16.3998 - val_loss: 0.2051 - val_mean_absolute_percentage_error: 7.3950\n",
      "Epoch 12/25\n",
      "7000/7000 [==============================] - 2s 239us/sample - loss: 0.3370 - mean_absolute_percentage_error: 16.2316 - val_loss: 0.1460 - val_mean_absolute_percentage_error: 5.4008\n",
      "Epoch 13/25\n",
      "7000/7000 [==============================] - 3s 384us/sample - loss: 0.3147 - mean_absolute_percentage_error: 15.4416 - val_loss: 0.1382 - val_mean_absolute_percentage_error: 5.3814\n",
      "Epoch 14/25\n",
      "7000/7000 [==============================] - 2s 255us/sample - loss: 0.3137 - mean_absolute_percentage_error: 15.0599 - val_loss: 0.1585 - val_mean_absolute_percentage_error: 5.8934\n",
      "Epoch 15/25\n",
      "7000/7000 [==============================] - 2s 232us/sample - loss: 0.3051 - mean_absolute_percentage_error: 14.6870 - val_loss: 0.1352 - val_mean_absolute_percentage_error: 5.0642\n",
      "Epoch 16/25\n",
      "7000/7000 [==============================] - 2s 240us/sample - loss: 0.2994 - mean_absolute_percentage_error: 14.6542 - val_loss: 0.1260 - val_mean_absolute_percentage_error: 4.7842\n",
      "Epoch 17/25\n",
      "7000/7000 [==============================] - 2s 247us/sample - loss: 0.3019 - mean_absolute_percentage_error: 14.7281 - val_loss: 0.1149 - val_mean_absolute_percentage_error: 4.4670\n",
      "Epoch 18/25\n",
      "7000/7000 [==============================] - 2s 234us/sample - loss: 0.2996 - mean_absolute_percentage_error: 14.7201 - val_loss: 0.1101 - val_mean_absolute_percentage_error: 4.2638\n",
      "Epoch 19/25\n",
      "7000/7000 [==============================] - 2s 231us/sample - loss: 0.2920 - mean_absolute_percentage_error: 14.2568 - val_loss: 0.1020 - val_mean_absolute_percentage_error: 4.0045\n",
      "Epoch 20/25\n",
      "7000/7000 [==============================] - 2s 231us/sample - loss: 0.3051 - mean_absolute_percentage_error: 14.8735 - val_loss: 0.1003 - val_mean_absolute_percentage_error: 3.9537\n",
      "Epoch 21/25\n",
      "7000/7000 [==============================] - 2s 229us/sample - loss: 0.2953 - mean_absolute_percentage_error: 14.3344 - val_loss: 0.1104 - val_mean_absolute_percentage_error: 4.3290\n",
      "Epoch 22/25\n",
      "7000/7000 [==============================] - 2s 230us/sample - loss: 0.2882 - mean_absolute_percentage_error: 13.9009 - val_loss: 0.1034 - val_mean_absolute_percentage_error: 4.0900\n",
      "Epoch 23/25\n",
      "7000/7000 [==============================] - 2s 233us/sample - loss: 0.2915 - mean_absolute_percentage_error: 14.1430 - val_loss: 0.1073 - val_mean_absolute_percentage_error: 4.2096\n",
      "Epoch 24/25\n",
      "7000/7000 [==============================] - 2s 228us/sample - loss: 0.2820 - mean_absolute_percentage_error: 13.5236 - val_loss: 0.0930 - val_mean_absolute_percentage_error: 3.6586\n",
      "Epoch 25/25\n",
      "7000/7000 [==============================] - 2s 242us/sample - loss: 0.2904 - mean_absolute_percentage_error: 14.0907 - val_loss: 0.1026 - val_mean_absolute_percentage_error: 3.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9538ef7b38>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data_tr,y_tr,batch_size=64,epochs=25,verbose=1,validation_data = (data_val,y_val),callbacks=[csv_logger,rlr,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Partner_Best_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model_data,model=model):\n",
    "    pred = model.predict(model_data)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tr = prediction(data_tr)\n",
    "pred_val = prediction(data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_list(arr):\n",
    "    listed = [item for sublist in arr for item in sublist]\n",
    "    return listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_dataframe(df,pred_tr,pred_val,y_tr,y_val):\n",
    "    pred_tr = array_to_list(pred_tr)\n",
    "    pred_val = array_to_list(pred_val)\n",
    "    preds = pred_tr + pred_val\n",
    "    actuals = list(y_tr) + list(y_val)\n",
    "    dr = pd.DataFrame()\n",
    "    dr['Date'] = df.Date\n",
    "    dr['ID'] = df.ID\n",
    "    dr['Actual'] = actuals\n",
    "    dr['Predict'] = preds\n",
    "    dr = dr.loc[:,['ID','Date','Actual','Predict']]\n",
    "    dr.to_pickle('Scaled_Predictions_Qtr.pkl')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_dataframe(df,pred_tr,pred_val,y_tr,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(y_val,pred_val):\n",
    "    mean_squared_error=mse(pred_val,y_val)\n",
    "    mean_absolute_error=mape(pred_val,y_val)\n",
    "    mean_absolute_percentage_error=(mape/y_val)*100\n",
    "    return mean_squared_error,mean_absolute_error,mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of code: Close this file using File 'Close and Halt' from dropdown menu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
